{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d78518ee",
   "metadata": {},
   "source": [
    "\n",
    "# DCGAN + SqueezeNet (Discriminador) con FashionMNIST\n",
    "\n",
    "Este cuadernillo implementa una **DCGAN** donde:\n",
    "- El **Generador** es una red deconvolucional clásica (salida 1×64×64 en `[-1, 1]`).\n",
    "- El **Discriminador** usa **SqueezeNet** *preentrenada en ImageNet* como *backbone*.\n",
    "  - Repetimos el canal (de 1 a 3) y **redimensionamos** a 224×224 para que\n",
    "    encaje con SqueezeNet.\n",
    "  - Reemplazamos la cabeza de clasificación por un único *logit* (real/falso).\n",
    "  - Opción de congelar características y afinar sólo la cabeza, o\n",
    "    *descongelar* gradualmente capas superiores.\n",
    "\n",
    "**Dataset:** `FashionMNIST` (28×28 gris). Si el *download* falla, el código puede alternar a `KMNIST` automáticamente.\n",
    "\n",
    "> **Nota:** Usar un backbone grande (SqueezeNet) para discriminar imágenes simples (64×64, 1 canal) es pedagógico y *funciona*, pero es más pesado que un discriminador DCGAN clásico. Gana robustez a texturas/patrones gracias al preentrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b690dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# ↑ Descomenta si estás en un entorno sin PyTorch (ajusta CUDA/CPU según tu caso).\n",
    "\n",
    "import os, math, time, random\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils as vutils, models\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ------------------------\n",
    "# Configuración principal\n",
    "# ------------------------\n",
    "cfg = {\n",
    "    \"data_root\": \"./data\",\n",
    "    \"batch_size\": 128,\n",
    "    \"num_workers\": 2,\n",
    "    \"image_size_g\": 64,      # tamaño del Generador (1x64x64)\n",
    "    \"nz\": 128,               # dimensión del vector latente z\n",
    "    \"ngf\": 64,               # canales base del Generador\n",
    "    \"lr_g\": 2e-4,\n",
    "    \"lr_d\": 1e-4,            # tip: D ligeramente más lento al usar backbone grande\n",
    "    \"beta1\": 0.5,\n",
    "    \"beta2\": 0.999,\n",
    "    \"epochs\": 20,\n",
    "    \"save_every\": 5,\n",
    "    \"samples_dir\": \"./samples_dcgan_sqz\",\n",
    "    \"chkpt_dir\": \"./checkpoints_dcgan_sqz\",\n",
    "    \"freeze_squeezenet\": True,    # comienza congelando el backbone\n",
    "    \"unfreeze_after\": 10,         # descongela a partir de esta época (None para nunca)\n",
    "    \"use_amp\": True               # entrenamiento mixto (si hay CUDA)\n",
    "}\n",
    "\n",
    "Path(cfg[\"samples_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "Path(cfg[\"chkpt_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "g_seed = 1337\n",
    "random.seed(g_seed)\n",
    "torch.manual_seed(g_seed)\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(g_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af352ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------\n",
    "# Dataset: FashionMNIST\n",
    "# ------------------------\n",
    "# El generador produce 1x64x64, así que escalamos reales a 64 y normalizamos a [-1, 1].\n",
    "# Para el discriminador (SqueezeNet) convertiremos en vuelo a 3x224x224.\n",
    "\n",
    "transform_64 = transforms.Compose([\n",
    "    transforms.Resize(cfg[\"image_size_g\"]),\n",
    "    transforms.CenterCrop(cfg[\"image_size_g\"]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])  # -> [-1, 1]\n",
    "])\n",
    "\n",
    "def get_dataset():\n",
    "    tried = []\n",
    "    try:\n",
    "        ds = datasets.FashionMNIST(cfg[\"data_root\"], train=True, download=True, transform=transform_64)\n",
    "        name = \"FashionMNIST\"\n",
    "        return ds, name\n",
    "    except Exception as e:\n",
    "        tried.append((\"FashionMNIST\", str(e)))\n",
    "        try:\n",
    "            ds = datasets.KMNIST(cfg[\"data_root\"], train=True, download=True, transform=transform_64)\n",
    "            name = \"KMNIST\"\n",
    "            return ds, name\n",
    "        except Exception as e2:\n",
    "            tried.append((\"KMNIST\", str(e2)))\n",
    "            raise RuntimeError(f\"No se pudo descargar ninguno de los datasets: {tried}\")\n",
    "\n",
    "train_ds, ds_name = get_dataset()\n",
    "print(\"Dataset:\", ds_name, \"Tamaño:\", len(train_ds))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=cfg[\"batch_size\"], shuffle=True,\n",
    "                          num_workers=cfg[\"num_workers\"], pin_memory=(device.type==\"cuda\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d657b05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------\n",
    "# Modelo: Generador DCGAN (salida 1x64x64)\n",
    "# ------------------------\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz=128, ngf=64, out_ch=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # nz x 1 x 1  -> (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(nz, ngf*8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf*8),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # (ngf*8) x 4 x 4 -> (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf*8, ngf*4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf*4),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # -> (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf*4, ngf*2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf*2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # -> (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(ngf*2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # -> out_ch x 64 x 64\n",
    "            nn.ConvTranspose2d(ngf, out_ch, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "G = Generator(cfg[\"nz\"], cfg[\"ngf\"], out_ch=1).to(device)\n",
    "print(G)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbecb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------\n",
    "# Discriminador con SqueezeNet preentrenada\n",
    "# ------------------------\n",
    "# - Repite canal 1->3 y reescala a 224x224 en el forward.\n",
    "# - Normaliza con medias/devs de ImageNet (esperado por SqueezeNet).\n",
    "# - Reemplaza la cabeza por un logit.\n",
    "class DiscriminatorSqueezeNet(nn.Module):\n",
    "    def __init__(self, freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        sqz = models.squeezenet1_1(weights=models.SqueezeNet1_1_Weights.IMAGENET1K_V1)\n",
    "        self.features = sqz.features  # extractor\n",
    "        # reemplazar clasificador por un conv 1x1 a 1 canal (logit)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Conv2d(512, 1, kernel_size=1),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        # congelar backbone si se desea\n",
    "        for p in self.features.parameters():\n",
    "            p.requires_grad = not freeze_backbone\n",
    "\n",
    "        # Imagenet stats\n",
    "        self.register_buffer(\"mean\", torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1))\n",
    "        self.register_buffer(\"std\",  torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1))\n",
    "\n",
    "    def preprocess_to_224(self, x_1x64x64):\n",
    "        # x: (B,1,64,64) en [-1,1]\n",
    "        x = (x_1x64x64 + 1)/2.0  # -> [0,1]\n",
    "        x = x.repeat(1,3,1,1)    # 1->3 canales\n",
    "        x = F.interpolate(x, size=(224,224), mode=\"bilinear\", align_corners=False)\n",
    "        # normalizar para SqueezeNet\n",
    "        x = (x - self.mean) / self.std\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.preprocess_to_224(x)\n",
    "        feat = self.features(x)           # (B,512,13,13)\n",
    "        out = self.classifier(feat)       # (B, 1*13*13) -> tras Flatten\n",
    "        # Global average pool implícito en SqueezeNet original, aquí usamos conv+flatten\n",
    "        # Opcional: hacer media espacial manual para dejar un solo logit por imagen:\n",
    "        out = out.mean(dim=1, keepdim=True)  # (B,1)\n",
    "        return out\n",
    "\n",
    "D = DiscriminatorSqueezeNet(freeze_backbone=cfg[\"freeze_squeezenet\"]).to(device)\n",
    "print(\"Parametros D (entrenables):\", sum(p.numel() for p in D.parameters() if p.requires_grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca385e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optG = optim.Adam(G.parameters(), lr=cfg[\"lr_g\"], betas=(cfg[\"beta1\"], cfg[\"beta2\"]))\n",
    "optD = optim.Adam(filter(lambda p: p.requires_grad, D.parameters()),\n",
    "                  lr=cfg[\"lr_d\"], betas=(cfg[\"beta1\"], cfg[\"beta2\"]))\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(cfg[\"use_amp\"] and device.type==\"cuda\"))\n",
    "\n",
    "def sample_fixed_grid(G, nz=128, nrow=8, fname=\"sample.png\"):\n",
    "    G.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(nrow*nrow, nz, 1, 1, device=device)\n",
    "        fakes = G(z).cpu()  # en [-1,1]\n",
    "        vutils.save_image((fakes+1)/2, fname, nrow=nrow)\n",
    "    G.train()\n",
    "\n",
    "def save_ckpt(epoch):\n",
    "    torch.save({\n",
    "        \"G\": G.state_dict(),\n",
    "        \"D\": D.state_dict(),\n",
    "        \"optG\": optG.state_dict(),\n",
    "        \"optD\": optD.state_dict(),\n",
    "        \"epoch\": epoch,\n",
    "        \"cfg\": cfg\n",
    "    }, os.path.join(cfg[\"chkpt_dir\"], f\"dcgan_sqz_ep{epoch}.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b13048",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "real_label = 1.0\n",
    "fake_label = 0.0\n",
    "\n",
    "fixed_out = os.path.join(cfg[\"samples_dir\"], \"fixed_grid_ep0.png\")\n",
    "sample_fixed_grid(G, cfg[\"nz\"], nrow=8, fname=fixed_out)\n",
    "print(\"Muestra inicial guardada en:\", fixed_out)\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(1, cfg[\"epochs\"]+1):\n",
    "    if cfg[\"unfreeze_after\"] is not None and epoch == cfg[\"unfreeze_after\"]:\n",
    "        # Descongelar backbone para *fine-tuning suave*\n",
    "        for p in D.features.parameters():\n",
    "            p.requires_grad = True\n",
    "        optD = optim.Adam(D.parameters(), lr=cfg[\"lr_d\"]*0.5, betas=(cfg[\"beta1\"], cfg[\"beta2\"]))\n",
    "        print(f\"[Epoch {epoch}] Descongelado parcial del backbone y lr_D reducido\")\n",
    "\n",
    "    for i, (imgs, _) in enumerate(train_loader):\n",
    "        bsz = imgs.size(0)\n",
    "        real = imgs.to(device, non_blocking=True)\n",
    "\n",
    "        # --------------------\n",
    "        # (1) Actualiza D\n",
    "        # --------------------\n",
    "        D.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=(cfg[\"use_amp\"] and device.type==\"cuda\")):\n",
    "            # Reales\n",
    "            logits_real = D(real)\n",
    "            labels_real = torch.full((bsz,1), real_label, device=device)\n",
    "            loss_D_real = criterion(logits_real, labels_real)\n",
    "\n",
    "            # Falsas\n",
    "            z = torch.randn(bsz, cfg[\"nz\"], 1, 1, device=device)\n",
    "            fake = G(z).detach()\n",
    "            logits_fake = D(fake)\n",
    "            labels_fake = torch.full((bsz,1), fake_label, device=device)\n",
    "            loss_D_fake = criterion(logits_fake, labels_fake)\n",
    "\n",
    "            loss_D = loss_D_real + loss_D_fake\n",
    "\n",
    "        scaler.scale(loss_D).backward()\n",
    "        scaler.step(optD)\n",
    "        scaler.update()\n",
    "\n",
    "        # --------------------\n",
    "        # (2) Actualiza G\n",
    "        # --------------------\n",
    "        G.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(cfg[\"use_amp\"] and device.type==\"cuda\")):\n",
    "            z = torch.randn(bsz, cfg[\"nz\"], 1, 1, device=device)\n",
    "            fake = G(z)\n",
    "            logits_fake_g = D(fake)\n",
    "            labels_real_for_g = torch.full((bsz,1), real_label, device=device)\n",
    "            loss_G = criterion(logits_fake_g, labels_real_for_g)\n",
    "\n",
    "        scaler.scale(loss_G).backward()\n",
    "        scaler.step(optG)\n",
    "        scaler.update()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"[{epoch:03d}/{cfg['epochs']}] step {i+1:04d}/{len(train_loader)} \"\n",
    "                  f\"lossD={loss_D.item():.3f} lossG={loss_G.item():.3f}\")\n",
    "\n",
    "    # Muestras y checkpoints por época\n",
    "    out_path = os.path.join(cfg[\"samples_dir\"], f\"fakes_ep{epoch}.png\")\n",
    "    sample_fixed_grid(G, cfg[\"nz\"], nrow=8, fname=out_path)\n",
    "    print(f\"Guardado grid de muestras: {out_path}\")\n",
    "\n",
    "    if epoch % cfg[\"save_every\"] == 0:\n",
    "        save_ckpt(epoch)\n",
    "\n",
    "elapsed = (time.time()-start_time)/60\n",
    "print(f\"Entrenamiento terminado en {elapsed:.1f} min.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86a645a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualización rápida de un batch real y uno falso\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_tensor_grid(x, title):\n",
    "    grid = vutils.make_grid((x[:64].cpu()+1)/2, nrow=8)  # asume [-1,1]\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.imshow(grid.permute(1,2,0))\n",
    "\n",
    "# batch real\n",
    "real_batch = next(iter(train_loader))[0].to(device)\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(64, cfg[\"nz\"], 1, 1, device=device)\n",
    "    fake_batch = G(z)\n",
    "\n",
    "show_tensor_grid(real_batch, f\"Reales ({ds_name})\")\n",
    "plt.show()\n",
    "\n",
    "show_tensor_grid(fake_batch, \"Falsas (G)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2e6e01",
   "metadata": {},
   "source": [
    "\n",
    "## Notas y recomendaciones\n",
    "\n",
    "- **Normalización**: El generador produce `[-1,1]`. Para SqueezeNet convertimos a `[0,1]`, replicamos canal y redimensionamos a `224×224`, luego normalizamos con las estadísticas de ImageNet.\n",
    "- **Estabilidad**: Al inicio conviene **congelar** el backbone y entrenar sólo la cabeza. Más adelante, podemos **descongelar** capas superiores (`unfreeze_after`) para ganar potencia.\n",
    "- **Ajustes útiles**:\n",
    "  - Subir `epochs` a 50–100.\n",
    "  - Bajar `batch_size` si tu GPU/CPU es limitada.\n",
    "  - Probar `lr_d` más bajo o usar *label smoothing* ligero para reales (p.ej. `0.9`).\n",
    "- **Alternativa**: Si prefieres un discriminador 100% DCGAN (ligero), reemplaza este por un CNN con *stride* y *LeakyReLU*. El objetivo de este cuaderno es mostrar cómo **reutilizar** un modelo **preentrenado** como discriminador.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}